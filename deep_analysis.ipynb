{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from icl.util_classes.arg_classes import DeepArgs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def get_auc_roc_score(\n",
        "    task_name,\n",
        "    seed=42,\n",
        "    sample_size=1000,\n",
        "    model_name=\"gpt2-xl\",\n",
        "    demonstration_shot=1,\n",
        "):\n",
        "    if model_name == \"gpt2-xl\":\n",
        "        num_layer = 48\n",
        "    elif model_name == \"gpt-j-6b\":\n",
        "        num_layer = 28\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    try:\n",
        "        args = DeepArgs(\n",
        "            task_name=task_name,\n",
        "            seeds=[seed],\n",
        "            sample_size=sample_size,\n",
        "            model_name=model_name,\n",
        "            using_old=False,\n",
        "            demonstration_shot=demonstration_shot,\n",
        "        )\n",
        "        y = args.load_result()[0][0]\n",
        "    except:\n",
        "        args = DeepArgs(\n",
        "            task_name=task_name,\n",
        "            seeds=[seed],\n",
        "            sample_size=sample_size,\n",
        "            model_name=model_name,\n",
        "            using_old=True,\n",
        "            demonstration_shot=demonstration_shot,\n",
        "        )\n",
        "        y = args.load_result()[0][0]\n",
        "\n",
        "    scores = []\n",
        "    gold = y.predictions[0].argmax(-1)\n",
        "    num_class = y.predictions[0].shape[-1]\n",
        "    select_list = []\n",
        "    for i in range(num_class):\n",
        "        if (gold == i).sum() > 0:\n",
        "            select_list.append(i)\n",
        "    for layer in range(0, num_layer):\n",
        "        if demonstration_shot == 1:\n",
        "            # the difference in implementation of demonstration_shot >=2 causes the difference in the order of layer and class,\n",
        "            # since we have run the experiments, we do not align the order in the implementation and just change the order here\n",
        "            pred = y.predictions[2].reshape(-1, num_layer, num_class)[\n",
        "                :, layer, :\n",
        "            ]\n",
        "        else:\n",
        "            pred = y.predictions[2].reshape(-1, num_class, num_layer)[\n",
        "                :, :, layer\n",
        "            ]\n",
        "        pred = pred[:, select_list]\n",
        "        if len(select_list) == 2:\n",
        "            pred = pred[:, 1]\n",
        "        else:\n",
        "            pred = pred / pred.sum(-1, keepdims=True)\n",
        "        scores.append(roc_auc_score(gold, pred, multi_class=\"ovo\"))\n",
        "    return scores\n",
        "\n",
        "\n",
        "def get_mean_auc_roc_score(\n",
        "    task_name,\n",
        "    seeds=None,\n",
        "    sample_size=1000,\n",
        "    model_name=\"gpt2-xl\",\n",
        "    demonstration_shot=1,\n",
        "):\n",
        "    if seeds is None:\n",
        "        seeds = [42, 43, 44, 45, 46]\n",
        "    score_list = []\n",
        "    for seed in seeds:\n",
        "        score_list.append(\n",
        "            get_auc_roc_score(\n",
        "                task_name,\n",
        "                seed,\n",
        "                sample_size,\n",
        "                model_name,\n",
        "                demonstration_shot=demonstration_shot,\n",
        "            )\n",
        "        )\n",
        "    return np.mean(score_list, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "demonstration_shot = 2\n",
        "model_name = \"gpt-j-6b\"\n",
        "\n",
        "task_names = [\"SST-2\", \"TREC\", \"AGNews\", \"EmoC\"]\n",
        "scores_list = [\n",
        "    get_mean_auc_roc_score(\n",
        "        task, model_name=model_name, demonstration_shot=demonstration_shot\n",
        "    )\n",
        "    for task in [\"sst2\", \"trec\", \"agnews\", \"emo\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(\n",
        "    scores_list,\n",
        "    open(f\"auc_roc_scores_{model_name}_{demonstration_shot}.pkl\", \"wb\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mpl.rcParams[\"text.usetex\"] = False\n",
        "scores = np.mean(np.array(scores_list), axis=0)\n",
        "\n",
        "normalized_scores = np.cumsum(scores - 0.5) / np.cumsum(scores - 0.5)[-1]\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(range(1, len(scores) + 1), scores, \"b--\")\n",
        "ax1.set_xlabel(\"Layers\")\n",
        "ax1.set_ylabel(r\"$\\mathrm{AUCROC_l}$\")\n",
        "ax1.tick_params(\"y\")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax2.plot(range(1, len(scores) + 1), normalized_scores, \"r-\")\n",
        "ax2.set_ylabel(r\"$R_l$\")\n",
        "ax2.tick_params(\"y\")\n",
        "\n",
        "ax1.legend([r\"$\\mathrm{AUCROC_l}$\"], loc=\"upper left\")\n",
        "ax2.legend([r\"$R_l$\"], loc=\"upper right\")\n",
        "\n",
        "plt.show()\n",
        "fig.savefig(\n",
        "    f\"AUC_ROC_{model_name}_{demonstration_shot}.pdf\",\n",
        "    dpi=300,\n",
        "    bbox_inches=\"tight\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "x = range(len(scores_list[0]))\n",
        "for task_name, scores in zip(task_names, scores_list):\n",
        "    ax.plot(x, scores, label=task_name)\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "ax.set_title(f\"AUC-ROC Score of {model_name} on Different Tasks\")\n",
        "ax.set_xlabel(\"Layer\")\n",
        "ax.set_ylabel(\"Score\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(\n",
        "    f\"AUC-ROC Score of {model_name} on Different Tasks (demonstration={demonstration_shot}).png\",\n",
        "    dpi=300,\n",
        "    bbox_inches=\"tight\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "x = range(len(scores_list[0]))\n",
        "for task_name, scores in zip(task_names, scores_list):\n",
        "    n_scores = np.cumsum(scores - 0.5)\n",
        "    n_scores = n_scores / n_scores[-1]\n",
        "    ax.plot(x, n_scores, label=task_name)\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "ax.set_title(f\"Prediction Ratio of {model_name} on Different Tasks\")\n",
        "ax.set_xlabel(\"Layer\")\n",
        "ax.set_ylabel(\"Ratio\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(\n",
        "    f\"Prediction Ratio of {model_name} on Different Tasks {demonstration_shot}.png\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.10 ('wm')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "69f52fabb15766d39c6bf90ba53c555c905cb082f5a671ecb5c4487727b3f015"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
